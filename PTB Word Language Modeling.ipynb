{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language Modeling Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Task : Given a sequence of words, predict the next word\n",
    "  - Models the probability of sentences in a language\n",
    "* Data available at http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \r\n",
      " mr. <unk> is chairman of <unk> n.v. the dutch publishing group \r\n",
      " rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \r\n"
     ]
    }
   ],
   "source": [
    "!head -n3 data/ptb_word/ptb.train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exactly 10000 words including `<unk>` and `<eos>` (End of sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words 10000\n"
     ]
    }
   ],
   "source": [
    "words = open('data/ptb_word/ptb.train.txt').read().replace('\\n', '<eos>').split()\n",
    "words_as_set = set(words)\n",
    "print('Number of words %d' % len(words_as_set))\n",
    "word_to_id = {w: i for i, w in enumerate(words_as_set)}\n",
    "id_to_word = {i: w for i, w in enumerate(words_as_set)}\n",
    "data = [word_to_id[w] for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's build the following model\n",
    "  - A recurrent neural network, unrolled in time\n",
    "  - Long short term memory (LSTM) cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='data/lstm.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* LSTM Cell\n",
    "  - Takes input, previous output and current state, and produces output and next state.\n",
    "  \n",
    "$$\n",
    "h_t, C_t = lstm(x_t, h_{t-1}, C_{t-1})\n",
    "$$\n",
    "\n",
    "<img src='data/lstm_cell.png' width='40%'>\n",
    "\n",
    "* Full set of equations ($[]$ is vector concatenation, $\\times$ is matrix multiply, $*$ is element-wise multiply)\n",
    "\n",
    "$$ X = [h_{t-1}, x_t] $$\n",
    "$$ f_t = \\sigma(W_f \\times X + b_f) $$\n",
    "$$ i_t = \\sigma(W_i \\times X + b_i) $$\n",
    "$$ o_t = \\sigma(W_o \\times X + b_o) $$\n",
    "$$ \\tilde{C}_t = tanh(W_C \\times X + b_C) $$\n",
    "$$ C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "$$ h_t = o_t * tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters of the model\n",
    "* We need to pick embedding dimensions and the dimensions of the state vector.\n",
    "  - For convenience, let's pick `embedding_dims = state_size = 128`\n",
    "* Embedding vectors\n",
    "  - `[10000, embedding_dims]`.\n",
    "* The 4 weight matrices in the equation ($W_f, W_i, W_o, W_C$)\n",
    "  - `[2 * state_size, state_size]`\n",
    "* 4 biases ($b_f, b_i, b_o, b_C$)\n",
    "  - `[state_size]`\n",
    "* Softmax classifier logit layer weights and biases\n",
    "  - `[state_size, 10000], [10000]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Implement an LSTM cell as a class, so we can instantiate many layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMCell(object):\n",
    "  def __init__(self, state_size):\n",
    "    self.state_size = state_size\n",
    "    self.W_f = tf.Variable(self.initializer())\n",
    "    self.W_i = tf.Variable(self.initializer())\n",
    "    self.W_o = tf.Variable(self.initializer())\n",
    "    self.W_C = tf.Variable(self.initializer())\n",
    "    self.b_f = tf.Variable(tf.zeros([state_size]))\n",
    "    self.b_i = tf.Variable(tf.zeros([state_size]))\n",
    "    self.b_o = tf.Variable(tf.zeros([state_size]))\n",
    "    self.b_C = tf.Variable(tf.zeros([state_size]))\n",
    "  def __call__(self, x_t, h_t1, C_t1):\n",
    "    X = tf.concat(1, [h_t1, x_t])\n",
    "    f_t = tf.sigmoid(tf.matmul(X, self.W_f) + self.b_f)\n",
    "    i_t = tf.sigmoid(tf.matmul(X, self.W_i) + self.b_i)\n",
    "    o_t = tf.sigmoid(tf.matmul(X, self.W_o) + self.b_o)\n",
    "    Ctilde_t = tf.tanh(tf.matmul(X, self.W_C) + self.b_C)\n",
    "    C_t = f_t * C_t1 + i_t * Ctilde_t\n",
    "    h_t = o_t * tf.tanh(C_t)\n",
    "    return h_t, C_t\n",
    "  def initializer(self):\n",
    "    return tf.random_uniform([2*self.state_size, self.state_size],\n",
    "                             -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Declare embedding vectors, LSTM cells, and logit layer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_size = 128\n",
    "\n",
    "embedding_params = tf.Variable(tf.random_uniform([10000, state_size],\n",
    "                                                 -0.02, 0.02))\n",
    "\n",
    "lstm = []\n",
    "for _ in range(4):\n",
    "  lstm.append(LSTMCell(state_size))\n",
    "\n",
    "sm_w = tf.Variable(tf.random_uniform([state_size, 10000], -0.1, 0.1))\n",
    "sm_b = tf.Variable(tf.zeros([10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's build the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words and targets are placeholders for [batch_size, num_steps]\n",
    "# tensor of word and target ids\n",
    "words = tf.placeholder(tf.int64, name='words')\n",
    "targets = tf.placeholder(tf.int64, name='targets')\n",
    "\n",
    "def model(batch_size, num_steps):\n",
    "  output = [tf.zeros([batch_size, state_size])] * 4\n",
    "  state = [tf.zeros([batch_size, state_size])] * 4\n",
    "  preds = []\n",
    "  cost = 0.0\n",
    "  for i in range(num_steps):\n",
    "    # Get the embedding for words\n",
    "    embedding = tf.nn.embedding_lookup(embedding_params, words[:, i])\n",
    "    # Run the LSTM cells\n",
    "    output[0], state[0] = lstm[0](embedding, output[0], state[0])\n",
    "    for d in range(1, 4):\n",
    "      output[d], state[d] = lstm[d](output[d-1], output[d], state[d])\n",
    "    # Get the logits\n",
    "    logits = tf.matmul(output[-1], sm_w) + sm_b\n",
    "    # Get the softmax predictions\n",
    "    preds.append(tf.nn.softmax(logits))\n",
    "    # Cost per step\n",
    "    cost = cost + tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\n",
    "                                                     targets[:, i]))\n",
    "  # Average cost across time steps\n",
    "  cost = cost / np.float32(num_steps)\n",
    "  return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Some boring routines to get mini-batch of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['the', 'company', \"'s\", 'egg'], ['company', \"'s\", 'egg', 'product'])\n"
     ]
    }
   ],
   "source": [
    "def get_one_example(num_steps):\n",
    "  offset = np.random.randint(len(data) - num_steps - 1)\n",
    "  return (data[offset:offset + num_steps],\n",
    "          data[offset+1:offset+1+num_steps])\n",
    "w, t = get_one_example(4)\n",
    "print([id_to_word[x] for x in w],[id_to_word[x] for x in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['baker', 'is', 'willing', 'to'], ['is', 'willing', 'to', 'accept'])\n",
      "(['N', 'N', 'to', 'yield'], ['N', 'to', 'yield', 'N'])\n"
     ]
    }
   ],
   "source": [
    "def get_mini_batch(batch_size, num_steps):\n",
    "  words, targets = [], []\n",
    "  for _ in range(batch_size):\n",
    "    w, t = get_one_example(num_steps)\n",
    "    words.append(w)\n",
    "    targets.append(t)\n",
    "  return np.array(words), np.array(targets)\n",
    "\n",
    "w, t = get_mini_batch(2, 4)\n",
    "for i in range(2):\n",
    "  print([id_to_word[x] for x in w[i]], [id_to_word[x] for x in t[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Everything in working order?\n",
    "* Try to get the predictions for a random example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\n",
      " 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001]\n"
     ]
    }
   ],
   "source": [
    "preds, cost = model(1, 8)\n",
    "tf.initialize_all_variables().run()\n",
    "w, t = get_mini_batch(1, 8)\n",
    "p = preds[0].eval(feed_dict={words: w, targets: t})\n",
    "np.set_printoptions(formatter={'float': lambda x: '%.04f'%x}, threshold=10000)\n",
    "print(p[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $e^{cost}$ should be approximately 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.2103415, 10000.011)\n"
     ]
    }
   ],
   "source": [
    "c = cost.eval(feed_dict={words: w, targets: t})\n",
    "print(c, np.exp(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's train the model\n",
    "* Let's get fancy\n",
    "  - Clip gradients before applying to parameters\n",
    "  - Use `tf.train.GradientDescentOptimizer` to reduce some boiler plate\n",
    "  - Use exponential decay on the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a variable to hold the step number, but mark it as not trainable \n",
    "global_step = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(learning_rate, batch_size, num_steps):\n",
    "  _, cost_value = model(batch_size, num_steps)\n",
    "  all_vars = tf.trainable_variables()\n",
    "  grads = tf.gradients(cost_value, all_vars)\n",
    "  grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "  # Decay the learning rate by 0.8 every 1000 steps\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    learning_rate, global_step, 1000, 0.8)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # apply_gradients increments the global_step\n",
    "  train_op = optimizer.apply_gradients(zip(grads, all_vars),\n",
    "                                       global_step=global_step)\n",
    "  return cost_value, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* And we are off to the races!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 9.210\n",
      "step 10: 9.067\n",
      "step 20: 8.898\n",
      "step 30: 8.349\n",
      "step 40: 7.647\n",
      "step 50: 7.557\n",
      "step 60: 7.284\n",
      "step 70: 7.101\n",
      "step 80: 7.090\n",
      "step 90: 7.010\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_timesteps = 16\n",
    "cost_value, train_op = train(1.0, batch_size, num_timesteps)\n",
    "tf.initialize_all_variables().run()\n",
    "for step_number in range(100):\n",
    "  w, t = get_mini_batch(batch_size, num_timesteps)\n",
    "  c, _ = sess.run([cost_value, train_op], feed_dict={words: w, targets: t})\n",
    "  if step_number % 10 == 0:\n",
    "    print('step %d: %.3f' % (step_number, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ptb_params-100'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess, './ptb_params', global_step=global_step.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's ask the model to generate sentences\n",
    "  - Start off with few words\n",
    "  - Sample from the probability distribution to get the next word\n",
    "  - Remember to feed the cell state back into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will donald trump casualty by tension major <eos> already\n",
      "what is not trust how slid davis in purchase\n",
      "the greatest soviet june N in asked the u.s.\n",
      "meaning of life reactions are but $ soon still\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, './ptb_params-5000')\n",
    "\n",
    "embedding = tf.nn.embedding_lookup(embedding_params, words[:, 0])\n",
    "output_in = [tf.zeros([1, state_size])] * 4\n",
    "state_in = [tf.zeros([1, state_size])] * 4\n",
    "output = [0] * 4\n",
    "state = [0] * 4\n",
    "# Run the LSTM cells\n",
    "output[0], state[0] = lstm[0](embedding, output_in[0], state_in[0])\n",
    "for d in range(1, 4):\n",
    "  output[d], state[d] = lstm[d](output[d-1], output_in[d], state_in[d])\n",
    "# Get the logits\n",
    "logits = tf.matmul(output[-1], sm_w) + sm_b\n",
    "# Get the softmax predictions\n",
    "preds = tf.nn.softmax(logits)\n",
    "\n",
    "def get_sentence(start_words, length):\n",
    "  start_words = start_words.split()\n",
    "  w = np.array([[word_to_id[start_words[0]]]])\n",
    "  t = sess.run([preds] + output + state,\n",
    "               feed_dict={words: w})\n",
    "  sentence = [start_words[0]]\n",
    "  for i in range(length):\n",
    "    if i + 1 < len(start_words):\n",
    "      w[0, 0] = word_to_id[start_words[i+1]]\n",
    "    else:\n",
    "      w[0, 0] = min(10000, np.sum(np.cumsum(t[0]) < np.random.rand()))\n",
    "    sentence.append(id_to_word[w[0, 0]])\n",
    "    feed_dict = dict([(output_in[i], t[1+i]) for i in range(4)]+\n",
    "                    [(state_in[i], t[5+i]) for i in range(4)] +\n",
    "                     [(words, w)])\n",
    "    t = sess.run([preds] + output + state,\n",
    "                 feed_dict=feed_dict)\n",
    "  return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will donald trump midnight esso present knowing resulting brady\n",
      "what is boat our requested aspects sweden robust reluctance\n",
      "the greatest yet lesser debris cnbc join jonathan idle\n",
      "meaning of life 14-year-old republics criticisms somebody inspector child\n"
     ]
    }
   ],
   "source": [
    "print(get_sentence('will donald trump', 8))\n",
    "print(get_sentence('what is', 8))\n",
    "print(get_sentence('the greatest', 8))\n",
    "print(get_sentence('meaning of life', 8))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
